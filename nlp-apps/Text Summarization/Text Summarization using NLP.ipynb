{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600ee1be",
   "metadata": {},
   "source": [
    "# **Text Summarization** [[readmore]](https://devopedia.org/text-summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b00c6-87c6-4e59-a445-422ddaf7aac8",
   "metadata": {},
   "source": [
    "# 1. Extractive Text Summarization\n",
    "\n",
    "Identifying important sections (paragraphs or sentences or even words) of the text and selecting to produce a subset of text from the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc63c96",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae51313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\raj\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f0e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.20.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (58.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7962bf92-eae1-4a5a-9f31-c0cbc3cd09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c3bdef-bec5-4446-9a04-80cf2bfacf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "770e3d26-d857-4489-94df-8aecafc09668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object of a pretrained NLP pipeline\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8b2298-0d3b-45d7-b37e-04f6771cb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Not very far in the past, Artificial Intelligence seemed a distant dream. Today, it’s pretty much everywhere, even though in its weakest forms. The AI industry is now worth $327.5 billion and is expected to grow up to $500 billion by 2024, according to a report by International Data Corporation (IDC). Though, the history of artificial intelligence is longer than we can imagine. \n",
    "\n",
    "Artificial Intelligence is defined as the development of a computer system that is able to perform a task that requires human intelligence, for example, speech recognition, visual perception, decision making, and translation between languages. A more lucid definition would be “the intelligence exhibited by machines or software”. It also refers to the academic field of study, which studies how to create computers and computer software capable of intelligent behavior. Since there is no clear line between ordinary software and AI, it becomes difficult to distinguish the difference. Hence, one may think of AI technology as the sort of technology one would use to perform tasks that require some level of intelligence to accomplish. \n",
    "\n",
    "The long-term goal of AI research ever since its inception has been what is called Strong AI, which describes a computer capable of doing every intelligent task that a human can perform. However, presently the AI innovation no more relies upon whether we accomplish Strong AI or not. The technology has now progressed to a level where, regardless of future advancements in the field, it is poised to disrupt any innovation-based industry. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ceedf-72de-45c5-98c1-32f4989e51e5",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92892b0-5853-4902-b290-07e2a92b5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f81eb14d-d7ed-4121-8d07-e6e14adc0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef997db0-03ee-401e-a33a-91524ace75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'Here', 'are', 'some', 'everyday', 'examples', 'of', 'text', 'summarization', ':', 'news', 'headlines', ',', 'outlines', 'for', 'students', ',', 'movie', 'previews', ',', 'meeting', 'minutes', ',', 'biographies', 'for', 'resumes', 'or', 'obituaries', ',', 'abridged', 'versions', 'of', 'books', ',', 'newsletter', 'production', ',', 'financial', 'research', ',', 'patent', 'research', ',', 'legal', 'contract', 'analysis', ',', 'tweeting', 'about', 'new', 'content', ',', 'chatbots', 'that', 'answer', 'questions', ',', 'email', 'summaries', ',', 'and', 'more', '.', '\\n\\n', 'When', 'Google', 'Search', 'presents', 'search', 'results', ',', 'some', 'entries', 'are', 'accompanied', 'by', 'auto', '-', 'generated', 'summaries', '.', 'Google', 'may', 'be', 'leveraging', 'a', 'knowledge', 'graph', 'for', 'this', 'purpose', '.', 'Google', \"'s\", 'approach', 'to', 'summarization', 'is', 'mainly', 'entity', 'centric', '.', 'Summarization', 'extends', 'to', 'timelines', 'and', 'events', 'about', 'entities', '.', '\\n\\n', 'Doctors', 'write', 'long', 'medical', 'notes', 'containing', 'nutritional', 'information', 'for', 'pregnant', 'mothers', '.', 'When', 'these', 'were', 'reduced', 'to', 'short', 'crisp', 'summaries', ',', 'pregnant', 'mothers', 'found', 'them', 'a', 'lot', 'easier', 'to', 'understand', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24badbdc-e3de-468b-97a8-6cc40d77c436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\n”“'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = punctuation + \"\\n\\n\" + '”' + '“'  \n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddaab2-8c25-4ff9-beb4-5c7095e2b9c0",
   "metadata": {},
   "source": [
    "### Text Cleaning & Word frequency calculation (or BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "543edc45-f019-47e5-8e61-2faaf46fb516",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "for w_token in doc:\n",
    "    if w_token.text.lower() not in stopwords and w_token.text.lower() not in punctuation:\n",
    "        if w_token.text not in word_frequencies.keys():\n",
    "            word_frequencies[w_token.text] = 1\n",
    "        else:\n",
    "            word_frequencies[w_token.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7659e881-cd07-4469-9d02-a3b6a4860c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'everyday': 1, 'examples': 1, 'text': 1, 'summarization': 2, 'news': 1, 'headlines': 1, 'outlines': 1, 'students': 1, 'movie': 1, 'previews': 1, 'meeting': 1, 'minutes': 1, 'biographies': 1, 'resumes': 1, 'obituaries': 1, 'abridged': 1, 'versions': 1, 'books': 1, 'newsletter': 1, 'production': 1, 'financial': 1, 'research': 2, 'patent': 1, 'legal': 1, 'contract': 1, 'analysis': 1, 'tweeting': 1, 'new': 1, 'content': 1, 'chatbots': 1, 'answer': 1, 'questions': 1, 'email': 1, 'summaries': 3, 'Google': 3, 'Search': 1, 'presents': 1, 'search': 1, 'results': 1, 'entries': 1, 'accompanied': 1, 'auto': 1, 'generated': 1, 'leveraging': 1, 'knowledge': 1, 'graph': 1, 'purpose': 1, 'approach': 1, 'mainly': 1, 'entity': 1, 'centric': 1, 'Summarization': 1, 'extends': 1, 'timelines': 1, 'events': 1, 'entities': 1, 'Doctors': 1, 'write': 1, 'long': 1, 'medical': 1, 'notes': 1, 'containing': 1, 'nutritional': 1, 'information': 1, 'pregnant': 2, 'mothers': 2, 'reduced': 1, 'short': 1, 'crisp': 1, 'found': 1, 'lot': 1, 'easier': 1, 'understand': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ce9c2f7-1a9d-4914-9d39-f1d35cddb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "002b7f67-c6e6-461a-a605-146a5593622f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "083b2b8b-eb15-4c70-999a-fcb4ba8b00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing word frequencies\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b85443-3b6b-4b27-a327-8e24684e7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'everyday': 0.3333333333333333, 'examples': 0.3333333333333333, 'text': 0.3333333333333333, 'summarization': 0.6666666666666666, 'news': 0.3333333333333333, 'headlines': 0.3333333333333333, 'outlines': 0.3333333333333333, 'students': 0.3333333333333333, 'movie': 0.3333333333333333, 'previews': 0.3333333333333333, 'meeting': 0.3333333333333333, 'minutes': 0.3333333333333333, 'biographies': 0.3333333333333333, 'resumes': 0.3333333333333333, 'obituaries': 0.3333333333333333, 'abridged': 0.3333333333333333, 'versions': 0.3333333333333333, 'books': 0.3333333333333333, 'newsletter': 0.3333333333333333, 'production': 0.3333333333333333, 'financial': 0.3333333333333333, 'research': 0.6666666666666666, 'patent': 0.3333333333333333, 'legal': 0.3333333333333333, 'contract': 0.3333333333333333, 'analysis': 0.3333333333333333, 'tweeting': 0.3333333333333333, 'new': 0.3333333333333333, 'content': 0.3333333333333333, 'chatbots': 0.3333333333333333, 'answer': 0.3333333333333333, 'questions': 0.3333333333333333, 'email': 0.3333333333333333, 'summaries': 1.0, 'Google': 1.0, 'Search': 0.3333333333333333, 'presents': 0.3333333333333333, 'search': 0.3333333333333333, 'results': 0.3333333333333333, 'entries': 0.3333333333333333, 'accompanied': 0.3333333333333333, 'auto': 0.3333333333333333, 'generated': 0.3333333333333333, 'leveraging': 0.3333333333333333, 'knowledge': 0.3333333333333333, 'graph': 0.3333333333333333, 'purpose': 0.3333333333333333, 'approach': 0.3333333333333333, 'mainly': 0.3333333333333333, 'entity': 0.3333333333333333, 'centric': 0.3333333333333333, 'Summarization': 0.3333333333333333, 'extends': 0.3333333333333333, 'timelines': 0.3333333333333333, 'events': 0.3333333333333333, 'entities': 0.3333333333333333, 'Doctors': 0.3333333333333333, 'write': 0.3333333333333333, 'long': 0.3333333333333333, 'medical': 0.3333333333333333, 'notes': 0.3333333333333333, 'containing': 0.3333333333333333, 'nutritional': 0.3333333333333333, 'information': 0.3333333333333333, 'pregnant': 0.6666666666666666, 'mothers': 0.6666666666666666, 'reduced': 0.3333333333333333, 'short': 0.3333333333333333, 'crisp': 0.3333333333333333, 'found': 0.3333333333333333, 'lot': 0.3333333333333333, 'easier': 0.3333333333333333, 'understand': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a50ab0e-3f47-4196-af9c-41345e07183c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920d22e-1491-432d-97fc-159071ab9cde",
   "metadata": {},
   "source": [
    "### Sentence Tokenization & Sentence score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64dba6ff-dbbe-4bb9-a2c4-10f80d97751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "## Sentence tokenization\n",
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57fdd3bf-f79e-4d1b-a3a7-e5db92cee270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some everyday examples of text summarization: news headlines, outlines for students, movie previews, meeting minutes, biographies for resumes or obituaries, abridged versions of books, newsletter production, financial research, patent research, legal contract analysis, tweeting about new content, chatbots that answer questions, email summaries, and more.\n",
      "\n",
      "\n",
      "****When Google Search presents search results, some entries are accompanied by auto-generated summaries.\n",
      "****Google may be leveraging a knowledge graph for this purpose.\n",
      "****Google's approach to summarization is mainly entity centric.\n",
      "****Summarization extends to timelines and events about entities.\n",
      "\n",
      "\n",
      "****Doctors write long medical notes containing nutritional information for pregnant mothers.\n",
      "****When these were reduced to short crisp summaries, pregnant mothers found them a lot easier to understand.\n",
      "\n",
      "****"
     ]
    }
   ],
   "source": [
    "for sent in sentence_tokens:\n",
    "    print(sent, end=\"\\n****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d89bbd8-bd99-4a22-9fa4-9e9b1ee604f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence score\n",
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f81c2b-c8cc-4b5b-a93e-b5d1eb6b4d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       " Here are some everyday examples of text summarization: news headlines, outlines for students, movie previews, meeting minutes, biographies for resumes or obituaries, abridged versions of books, newsletter production, financial research, patent research, legal contract analysis, tweeting about new content, chatbots that answer questions, email summaries, and more.\n",
       " : 13.333333333333336,\n",
       " When Google Search presents search results, some entries are accompanied by auto-generated summaries.: 3.6666666666666665,\n",
       " Google may be leveraging a knowledge graph for this purpose.: 1.3333333333333333,\n",
       " Google's approach to summarization is mainly entity centric.: 1.9999999999999998,\n",
       " Summarization extends to timelines and events about entities.\n",
       " : 1.9999999999999998,\n",
       " Doctors write long medical notes containing nutritional information for pregnant mothers.: 3.666666666666666,\n",
       " When these were reduced to short crisp summaries, pregnant mothers found them a lot easier to understand.: 4.666666666666666}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef58d3-0065-414a-8f16-c70391296749",
   "metadata": {},
   "source": [
    "### Summarizing Text (extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a742e78-97a3-4982-b0dc-a6a8eaa7560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd5a26d5-1242-4ea1-a6c4-9a370f8edccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_length = int(len(sentence_tokens)*0.3)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e47630c-410a-4eb8-a9ce-8ed0c3a7cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " Here are some everyday examples of text summarization: news headlines, outlines for students, movie previews, meeting minutes, biographies for resumes or obituaries, abridged versions of books, newsletter production, financial research, patent research, legal contract analysis, tweeting about new content, chatbots that answer questions, email summaries, and more.\n",
       " ,\n",
       " When these were reduced to short crisp summaries, pregnant mothers found them a lot easier to understand.]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79fd190f-62ab-4a05-a5b1-c18f4705f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = [word.text for word in summary]\n",
    "summary = \" \".join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3b4c1c2-fcd3-4811-b2b9-73dc0a3e5b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some everyday examples of text summarization: news headlines, outlines for students, movie previews, meeting minutes, biographies for resumes or obituaries, abridged versions of books, newsletter production, financial research, patent research, legal contract analysis, tweeting about new content, chatbots that answer questions, email summaries, and more. When these were reduced to short crisp summaries, pregnant mothers found them a lot easier to understand.\n"
     ]
    }
   ],
   "source": [
    "summary = summary.replace(\"\\n\",\"\").replace(\"  \", \" \")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b0b7f",
   "metadata": {},
   "source": [
    "# 2. Abstractive Text Summarization using `pegasus-xsum`\n",
    "Technique of generating a summary of a text from its main ideas, not by copying the verbatim salient sentences from text.\n",
    "**Remark:** `pegasus-xsum` is a pretrained Seq2Seq Model that specializes in *text summarization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7919b",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "689fcbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  8 21:16:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.95       Driver Version: 512.95       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P0    12W /  N/A |    101MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5112    C+G   ...cd70vzyy\\ArmouryCrate.exe    N/A      |\n",
      "|    0   N/A  N/A      5880    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14380    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     14636    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17116    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     17592    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     17908    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     18000    C+G   ...661.62\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     19764    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20888    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     23992    C+G   ...ontend\\Docker Desktop.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Checking the hightest CUDA version supported by PC\n",
    "# https://stackoverflow.com/questions/9727688/how-to-get-the-cuda-version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8beb11",
   "metadata": {},
   "source": [
    "**Hightest version of CUDA supported is 11.6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b06cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu116\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp39-cp39-win_amd64.whl (2434.0 MB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp39-cp39-win_amd64.whl (4.8 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp39-cp39-win_amd64.whl (2.3 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\raj\\anaconda3\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\raj\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\raj\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.13.1+cu116 torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "# Installing Pytorch with CUDA 11.6\n",
    "# https://pytorch.org/get-started\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7747e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
     ]
    }
   ],
   "source": [
    "# Install HuggingFace Transformer\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3bb2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "# PegasusTokenizer requires the SentencePiece\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aedd09",
   "metadata": {},
   "source": [
    "### Importing and loading pre-trained Model from `huggingface_hub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff57197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies from transformers\n",
    "# https://huggingface.co/google/pegasus-xsum\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be5abac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a189bc2c5d7547b7817520caeb33b229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Raj\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccee145e7f9434397ad66274ae97ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cd7f084b8e4404aca9fc683105c4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2a935f882c47d29eca76ecabbab364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading tokenizer and leveraging the existing tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19d6c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0b1ad4d91b48e5b2559f21ae50bd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79e4afd3c5c498e8f8cf92028d15748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34319557",
   "metadata": {},
   "source": [
    "### Summarizing Text (Abstractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8563cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''We introduce a new language representation model called BERT, which stands for\n",
    "Bidirectional Encoder Representations from\n",
    "Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from\n",
    "unlabeled text by jointly conditioning on both\n",
    "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
    "to create state-of-the-art models for a wide\n",
    "range of tasks, such as question answering and\n",
    "language inference, without substantial taskspecific architecture modifications.\n",
    "BERT is conceptually simple and empirically\n",
    "powerful. It obtains new state-of-the-art results on eleven natural language processing\n",
    "tasks, including pushing the GLUE score to\n",
    "80.5% (7.7% point absolute improvement),\n",
    "MultiNLI accuracy to 86.7% (4.6% absolute\n",
    "improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n",
    "(5.1 point absolute improvement).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9a79bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokens - number representation of source text\n",
    "# Input: RAW Source Text\n",
    "# Output: pytorch tensors\n",
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cde9d2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  184,  4094,   114,   177,  1261,  5114,   861,   568,   110, 62613,\n",
       "           108,   162,  3034,   118,  7671, 37390, 93789, 37955,   116,   135,\n",
       "         38979,   107,  6857,   909,  1261,  5114,  1581,   143, 15617,   116,\n",
       "          3256,  2700,   107,   108,   931,   304,   206, 66506,  3256,  2700,\n",
       "           107,   108, 28505,   108,   110, 62613,   117,   638,   112,  1133,\n",
       "         20098,  1355, 79050, 13872,   135,  1596, 53541,  1352,   141, 13039,\n",
       "          5542,   124,   302,   518,   111,   268,  2956,   115,   149,  4427,\n",
       "           107,   398,   114,   711,   108,   109,  1133,   121, 14787,   110,\n",
       "         62613,   861,   137,   129,  1226, 37126,   122,   188,   156,   853,\n",
       "          2940,  2865,   112,   421,   449,   121,  1313,   121,   544,   121,\n",
       "          3904,  1581,   118,   114,   827,   499,   113,  2722,   108,   253,\n",
       "           130,   906,  9421,   111,  1261, 40410,   108,   347,  4844,  1778,\n",
       "          7115,  3105,  9896,   107,   110, 62613,   117, 64089,   586,   111,\n",
       "         60458,  1512,   107,   168,  2426,   116,   177,   449,   121,  1313,\n",
       "           121,   544,   121,  3904,   602,   124, 13066,   710,  1261,  2196,\n",
       "          2722,   108,   330,  5598,   109, 19295, 28012,  2135,   112,   608,\n",
       "         71400,   143, 84899,   491,  4448,  2757,   312,  5224, 21297,   187,\n",
       "          4426,   112,   608, 64959,   143, 72359,  4448,  2757,   312,   520,\n",
       "         60909, 11130,  2294, 17593,   906,  9421,  4447,  1091,   740,   112,\n",
       "           950, 29167, 57793,   491,  4448,  2757,   158,   111,   520, 60909,\n",
       "         11130,  2294, 19402,  4447,  1091,   740,   112,   608, 24989,   143,\n",
       "         33494,   491,  4448,  2757,   250,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83f51e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Summarize text\n",
    "summary = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05127849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  222,  136,  800,  108,  145,  799,  114,  177,  449,  121, 1313,\n",
       "          121,  544,  121, 3904,  861,  118, 1355, 1261, 2196,  107,    1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenized summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5908ad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>In this paper, we present a new state-of-the-art model for deep language processing.</s>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoded Summary\n",
    "tokenizer.decode(summary[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
